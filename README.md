# AI-Learning-to-Retrieve-and-Generate-Educational-Videos-with-Equitable-Ambient-Cues
The goal of the project is to develop artificial intelligence technology to retrieve and classify videos based on content and environmental cues in the context of STEM education in primary schools.
# The Task
My task was to use AI to retrieve and classify videos according to their content and environmental clues, and AI-generated the desired task image for teaching. First, environmental cues are a person's perception of cues related to their identity. Our aim is to teach equity, which is divided into race, gender, and age. The STEM research direction is science and technology, engineering, mathematics, and so on. I first need to crawl relevant videos from the website, then classify them into three categories.  By using some deep learning knowledge, I try to get video analysis code for face detection, classification, and video annotations. Following, I will mainly elaborate on my learning process through a literature review, approach, result, and future work.

# Code Description
# common.py
By adding two coordinate channels as extra channels in the traditional convolutional network to represent the x and y coordinates of the original input and connecting them with the original feature map, the convolutional process can be enhanced to perceive the spatial information of the Feature map, which can improve the performance of the network in image processing tasks.
The traditional convolution operation can only utilize the information within the local receptive field in the feature extraction process, and cannot directly perceive the position of each pixel point in the whole Feature map. By introducing coordinate channel, the convolution process can obtain the position information of each pixel point, which increases the ability of the convolution network to perceive the spatial structure and position-related features of the Feature map.
By connecting the coordinate information with the original feature map, the network can better capture the spatial structure information in the image, thus improving the accuracy of target localization and segmentation. After combining the spatial information, the relationship between pixel points at different locations in the image can be understood more accurately, which improves the network's ability to perceive the details of the target's shape, edges, etc., and improves the accuracy of detection.

# metrics.py
The Wise-IOU loss function introduces a dynamic non-monotonic focusing mechanism, which is used to assess the quality of anchor frames and adjust the corresponding gradient gain assignment strategy. Conventional focusing mechanisms usually use the IOU (Intersection over Union) loss function as a criterion for quality assessment, i.e., the quality of the anchor frames is assessed by calculating the degree of overlap between the predicted bounding box and the real bounding box. However, IOU only considers the positional overlap of the bounding box, which may not be sensitive enough for some special cases (e.g., object shape, size, etc.) of anchor frames.
The dynamic non-monotonic focusing mechanism introduces an "outlier" metric as an alternative to IOU for anchor frame quality assessment. Outlier measures the difference between the predicted bounding box and the target distribution. By introducing the outlier as a quality assessment metric, the shape and texture of the target can be considered more comprehensively, which results in better adaptability to targets of different shapes and sizes.
In addition, the dynamic non-monotonic focusing mechanism provides a judicious gradient gain assignment strategy. By reducing the competitiveness of high-quality anchor frames, decreasing their gradient weights, and reducing the harmful gradients generated by low-quality examples, the importance of different quality anchor frames can be effectively balanced. As a result, the dynamic non-monotonic focusing mechanism can focus more on the common quality anchor frames, thus improving the detection accuracy.The Wise-IOU loss function is calculated as follows:
Where, denotes the number of detection frames, Table is the coordinates of detection frames, denotes the coordinates of the real labeling frame of the ith object, denotes the IOU value between the ith object frame and the real labeling frame, and denotes the weight. After the introduction of the Wise-IOU loss function, the detection accuracy is significantly improved.

# ShuffleAttention.py
In order to simultaneously satisfy the requirements of speed, accuracy, and to reduce the computational overhead in the face detection task, the Shuffle Attention mechanism module built on the channel shuffle operator is chosen. This module is able to accurately focus on all relevant elements in the input while maintaining lightweight.
The SA module divides the feature mapping of the input into groups and integrates channel attention and spatial attention into one block of each group using Shuffle units. After that, all sub-features are aggregated and the "channel shuffle" operator is used to communicate information between different sub-features.
The SA module accurately captures and focuses on the important features of the face region while maintaining computational efficiency by dividing the input feature mapping into multiple groups and integrating the channel attention and spatial attention into one block of each group through the Shuffle unit. Meanwhile, by using the "channel shuffle" operator, the information between different sub-features can be communicated and interacted, which enhances the characterization ability of the module.
Experiments have shown that the introduction of the SA module effectively improves the accuracy of face detection and maintains the computational efficiency.

# yolo.py
YOLOv5s utilizes a lightweight backbone network with CSPDarknet53. This network structure reduces the number of parameters and improves computational efficiency while maintaining accuracy. And YOLOv5s introduces an adaptive selection strategy to pick more representative anchors to further improve the detection accuracy. In the training phase, YOLOv5 adopts new data enhancement methods, including random image enhancement and adaptive label smoothing to make the model have better generalization ability, which can effectively deal with targets of various scales, shapes, and angles.YOLOv5s introduces an integration strategy based on ESWA (Ensemble Soft Weighted Averaging), which is used to fuse multiple different versions of the model. for fusing multiple different versions of the model to further improve detection performance and robustness.
YOLOv5s consists of Backbone, Neck, Head, and Loss functions. Adding an attention mechanism to the Backbone and Head modules at appropriate locations can greatly improve the detection accuracy.
